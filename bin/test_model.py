#!/usr/bin/env python

import argparse
import pandas as pd
import numpy as np
from joblib import load
from sklearn import preprocessing
from os import path
from tensorflow import keras


def predict_class(model, eval_data):
    """
    Predict classes with tensorflow model using evaluation data
    :param model: tensorflow model
    :param eval_data: examples of data to classify
    :return: list with predicted classes
    """
    prediction = model.predict(eval_data)

    return [np.argmax(pred_probability) for pred_probability in prediction]


def predict_regx(model, eval_data):
    """
    Predict values using trained model
    :param model: tensorflow model
    :param eval_data: data for evaluation with no label
    :return: predicted label for data
    """
    return model.predict(eval_data).flatten()


# TODO preguntar que es posible que se cargen multiples veces el mismo modelo afectando el analisis
def main():
    args = parse_arguments()
    model_list = args.models
    output = args.output

    # Load and scale dataset
    test_df = pd.read_csv(args.test_dataset, sep=',', index_col=0).drop('response', axis=1)
    # TODO preguntar por que se escala en dataset, si ya fue escalado en prepare_dataset_to_train.py
    min_max_scaler = preprocessing.MinMaxScaler()
    validation_scaler = min_max_scaler.fit_transform(test_df)

    # Test models and save results
    response_list = []
    index = []

    for model_path in model_list:
        basename = path.basename(model_path)
        root, ext = path.splitext(basename)
        response_predict = None

        if ext == '.joblib':
            model = load(model_path)
            response_predict = model.predict(validation_scaler)

        elif ext == '.h5' and args.problem == 'classification':
            model = keras.models.load_model(model_path)
            response_predict = predict_class(model, validation_scaler)

        elif ext == '.h5' and args.problem == 'regression':
            model = keras.models.load_model(model_path)
            response_predict = predict_regx(model, validation_scaler)

        response_list.append(response_predict)
        index.append(basename)

    df = pd.DataFrame(response_list, index=index)
    df.to_csv(output, sep=',', index=True, float_format='%.5f')


def parse_arguments():
    """
    Parse input arguments of script

    @return: arguments parser
    """

    parser = argparse.ArgumentParser(
        "Test and save every model generated by training_class_models.py"
    )

    parser.add_argument(
        "-t",
        "--test-dataset",
        action="store",
        required=True,
        help="csv file with test dataset",
    )

    parser.add_argument(
        "-m",
        "--models",
        action="store",
        nargs='+',
        required=True,
        help="1 or more models saved as .joblib file",
    )

    parser.add_argument(
        "-e",
        "--encoding",
        action="store",
        help="encoding used on the input dataset",
    )

    parser.add_argument(
        "-p",
        "--problem",
        action="store",
        required=True,
        choices=['regression', 'classification'],
        help="type of problem {classification|regression}",
    )

    parser.add_argument(
        "-o",
        "--output",
        action="store",
        required=True,
        help="output path for csv file with testing results of models",
    )

    return parser.parse_args()


if __name__ == '__main__':
    main()
