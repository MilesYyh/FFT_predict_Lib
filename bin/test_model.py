#!/usr/bin/env python

import argparse
import pandas as pd
from joblib import dump, load
from sklearn import preprocessing
from os import path


def main():
    args = parse_arguments()
    model_list = args.models
    output = args.output

    # Load and scale dataset
    test_df = pd.read_csv(args.test_dataset, sep=',', index_col=0).drop('response', axis=1)
    min_max_scaler = preprocessing.MinMaxScaler()
    validation_scaler = min_max_scaler.fit_transform(test_df)

    # Test models and save results
    response_list = []
    index = []

    try:
        for model_path in model_list:
            model = load(model_path)
            response_predict = model.predict(validation_scaler)
            response_list.append(response_predict)
            index.append(path.basename(model_path))

        df = pd.DataFrame(response_list, index=index)
        df.to_csv(output, sep=',', index=True)
    except Exception as e:
        print('Error: ', e)


def parse_arguments():
    """
    Parse input arguments of script

    @return: arguments parser
    """

    parser = argparse.ArgumentParser(
        "Test and save every model generated by training_class_models.py"
    )

    parser.add_argument(
        "-t",
        "--test-dataset",
        action="store",
        required=True,
        help="csv file with test dataset",
    )

    parser.add_argument(
        "-m",
        "--models",
        action="store",
        nargs='+',
        required=True,
        help="1 or more models saved as .joblib file",
    )

    parser.add_argument(
        "-e",
        "--encoding",
        action="store",
        help="encoding used on the input dataset",
    )

    parser.add_argument(
        "-o",
        "--output",
        action="store",
        required=True,
        help="output path for csv file with testing results of models",
    )

    return parser.parse_args()


if __name__ == '__main__':
    main()
